# ğŸ¥ Real-Time Video Captioning System

> ğŸ¤– AI-powered live video captioning with BLIP model integration

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Flask](https://img.shields.io/badge/Flask-2.0+-green.svg)](https://flask.palletsprojects.com)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A sophisticated AI-powered application that generates live captions for webcam video streams using the BLIP (Bootstrapping Language-Image Pre-training) model with advanced optimization techniques.

---

## âœ¨ Features

<details>
<summary>ğŸ”¥ Core Functionality</summary>

- ğŸ“¹ **Real-time Video Processing**: Live webcam feed with continuous caption generation
- ğŸ§  **AI-Powered Captioning**: BLIP model integration for accurate scene description  
- âš¡ **Smart Frame Processing**: Adaptive frame skipping and scene change detection
- ğŸš€ **Performance Optimization**: Advanced caching, threading, and memory management
- ğŸ¯ **Scene Change Detection**: Intelligent caption updates based on visual changes
</details>

<details>
<summary>ğŸ¨ User Interface</summary>

- ğŸ“º **Live Video Overlay**: Real-time captions displayed over video feed
- ğŸ“Š **Performance Metrics**: FPS counter, latency tracking, confidence scores
- ğŸ“œ **Caption History**: Chronological list of generated captions
- ğŸ›ï¸ **Interactive Controls**: Start/stop, settings, audio narration, fullscreen
- ğŸ“± **Responsive Design**: Modern dark theme with mobile optimization
</details>

<details>
<summary>ğŸ”§ Advanced Features</summary>

- ğŸ”Š **Audio Narration**: Text-to-speech for generated captions
- ğŸ§© **Contextual Processing**: Caption enhancement with temporal awareness
- âš™ï¸ **Configurable Settings**: Adjustable frame rate, quality, and thresholds
- ğŸ’¾ **Smart Caching**: LRU cache system for performance optimization
</details>

---

## ğŸ› ï¸ Technology Stack

### ğŸ–¥ï¸ Backend
```
ğŸ Python Flask + SocketIO    â†’ Real-time server
ğŸ¤– BLIP Model                â†’ AI caption generation  
âš¡ PyTorch + CUDA            â†’ GPU-accelerated inference
ğŸ¯ OpenCV + PIL              â†’ Image processing
ğŸ§µ Thread Pool Executor      â†’ Concurrent processing
```

### ğŸŒ Frontend  
```
ğŸŒŸ HTML5 + CSS3 + JavaScript â†’ Modern web interface
ğŸ”Œ Socket.IO Client          â†’ Real-time communication
ğŸ“¹ WebRTC getUserMedia       â†’ Video capture
ğŸ¨ Custom Responsive Design  â†’ Glassmorphism UI
```

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

<details>
<summary>Click to expand requirements</summary>

- ğŸ Python 3.8+
- ğŸ® CUDA-compatible GPU (recommended)
- ğŸŒ Modern web browser with WebRTC support
- ğŸ“¹ Webcam access

</details>

### âš¡ Installation

```bash
# ğŸ“‚ Clone the repository
git clone https://github.com/Varsha-1605/Real-Time-Video-Captioning-Project.git
cd Real-Time-Video-Captioning-Project

# ğŸ“¦ Install dependencies
pip install -r requirements.txt

# ğŸš€ Run the application
python app.py

# ğŸŒ Open browser
# Navigate to http://localhost:5000
```

### ğŸ“¦ Dependencies
<details>
<summary>View full requirements.txt</summary>

```txt
numpy                    # ğŸ”¢ Numerical computing
torch                    # ğŸ”¥ Deep learning framework
torchvision             # ğŸ‘ï¸ Computer vision utilities
flask                   # ğŸŒ Web framework
flask_socketio          # âš¡ Real-time communication
pillow                  # ğŸ–¼ï¸ Image processing
opencv-python           # ğŸ“¹ Computer vision
transformers            # ğŸ¤— NLP models
accelerate              # âš¡ Model acceleration
opencv-contrib-python   # ğŸ“¹ Extended OpenCV
ultralytics             # ğŸ¯ YOLO models
gunicorn               # ğŸ¦„ WSGI server
python-engineio        # ğŸ”Œ Engine.IO server
gevent-websocket       # ğŸŒ WebSocket support
opencv-python-headless # ğŸ“¹ Headless OpenCV
```

</details>

---
## ğŸ“¸ Demo

### ğŸ–¼ï¸ Application Screenshots

<div align="center">

#### ğŸ¬ Main Interface
![Main Interface](images/video_img1.jpeg)
*Real-time video captioning with live webcam feed and caption overlay*

#### ğŸ“Š Performance Dashboard
![Performance Dashboard](images/video_img2.jpeg)
*Monitor FPS, latency, and caption confidence in real-time*

#### âš™ï¸ Settings Panel
![Settings Panel](images/video_img4.jpeg)
*Customize frame rate, quality, and captioning preferences*

#### ğŸ“Š logs
![Settings Panel](images/video_img5.jpeg)

</div>

---

## ğŸ® Usage Guide

### ğŸ¯ Getting Started

<details>
<summary>Step-by-step guide</summary>

1. ğŸš€ Launch the application and open the web interface
2. ğŸ“¹ Allow webcam permissions when prompted  
3. â–¶ï¸ Click "Start Analysis" to begin real-time captioning
4. ğŸ‘€ View live captions overlaid on your video feed
5. ğŸ“Š Monitor performance metrics in the sidebar

</details>

### âŒ¨ï¸ Keyboard Shortcuts

| Key | Action | Description |
|-----|--------|-------------|
| `Space` | â¯ï¸ Start/Stop | Toggle analysis on/off |
| `S` | âš™ï¸ Settings | Open settings modal |
| `M` | ğŸ”Š Mute | Toggle audio narration |
| `F` | â›¶ Fullscreen | Enter/exit fullscreen |

### ğŸ”§ Settings Configuration

<details>
<summary>Customize your experience</summary>

- ğŸ“Š **Frame Rate**: 10-30 FPS (adjusts processing frequency)
- ğŸ¨ **Image Quality**: 30-90% (compression level for transmission)  
- ğŸ”Š **Audio Narration**: Enable/disable text-to-speech
- ğŸ¯ **Confidence Threshold**: Minimum confidence for caption updates

</details>

---

## ğŸ—ï¸ Architecture

### ğŸ”„ System Design
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ğŸ“¡ WebSocket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸŒ Frontend   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚   ğŸ–¥ï¸ Backend     â”‚
â”‚   (Browser)     â”‚                    â”‚   (Flask+BLIP)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“¹ Video Captureâ”‚                    â”‚ ğŸ¯ Frame Process â”‚
â”‚ ğŸ¨ UI Updates   â”‚                    â”‚ ğŸ¤– AI Inference  â”‚
â”‚ ğŸ›ï¸ User Controlsâ”‚                    â”‚ ğŸ’¾ Caching       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§© Key Components

<details>
<summary>ğŸ–¥ï¸ Backend (app.py)</summary>

- ğŸ“¹ **Frame Processing Pipeline**: Smart buffering and preprocessing
- ğŸ¤– **BLIP Model Integration**: Optimized inference with GPU acceleration  
- ğŸ“Š **Performance Monitoring**: Real-time metrics collection
- ğŸ’¾ **Caching System**: LRU cache for repeated scenes
- ğŸ§µ **Thread Pool**: Concurrent processing for scalability

</details>

<details>
<summary>ğŸŒ Frontend Components</summary>

- ğŸ“¹ **Video Handler**: Webcam capture and frame extraction
- ğŸ”Œ **Socket Manager**: Real-time communication with backend
- ğŸ¨ **UI Controller**: Dynamic interface updates and user interactions  
- ğŸ“Š **Performance Tracker**: Client-side metrics and monitoring

</details>

### ğŸ”„ Processing Flow

```mermaid
graph TD
    A[ğŸ“¹ Video Capture] --> B[ğŸ¨ Preprocessing]
    B --> C[ğŸ“¡ Transmission]
    C --> D[ğŸ¤– AI Processing]
    D --> E[ğŸ¯ Smart Filtering]
    E --> F[ğŸ“¤ Response]
    F --> G[ğŸ¨ UI Update]
```

1. ğŸ“¹ **Video Capture**: Extract frames from webcam at configurable rate
2. ğŸ¨ **Preprocessing**: Resize, enhance, and encode frames
3. ğŸ“¡ **Transmission**: Send frame data via WebSocket
4. ğŸ¤– **AI Processing**: BLIP model generates caption with confidence
5. ğŸ¯ **Smart Filtering**: Scene change detection and caption optimization
6. ğŸ“¤ **Response**: Return caption with metadata to frontend
7. ğŸ¨ **UI Update**: Display caption with performance metrics

---

## âš™ï¸ Configuration

### ğŸŒ Environment Variables
```bash
# ğŸ¯ Optional: Customize server settings
FLASK_ENV=production
CUDA_VISIBLE_DEVICES=0
```

### ğŸš€ Performance Tuning

<details>
<summary>Advanced configuration options</summary>

```python
# ğŸ“Š Core Settings
FRAME_SKIP = 3                    # ğŸ“¹ Frame processing frequency
IMAGE_SIZE = 224                  # ğŸ–¼ï¸ Model input resolution  
CACHE_SIZE = 500                  # ğŸ’¾ Number of cached captions
MAX_WORKERS = 6                   # ğŸ§µ Thread pool size

# ğŸ”§ Advanced Settings
ADAPTIVE_QUALITY = True           # ğŸ¨ Enable image enhancement
MIN_PROCESSING_INTERVAL = 0.1     # â±ï¸ Minimum time between frames
SCENE_CHANGE_THRESHOLD = 0.15     # ğŸ¯ Sensitivity for scene detection
CAPTION_HISTORY_SIZE = 10         # ğŸ“œ Context buffer size
```

</details>

---

## ğŸ“Š Performance Metrics

### âš¡ Typical Performance
| Metric | Value | Description |
|--------|-------|-------------|
| ğŸ•’ **Processing Latency** | 200-800ms | Per frame processing time |
| ğŸ“Š **Frame Rate** | 5-15 FPS | Adaptive based on hardware |
| ğŸ’¾ **Memory Usage** | ~800MB RAM, ~2GB GPU | System resources |
| âš™ï¸ **CPU Utilization** | 15-30% | On modern systems |
| ğŸ“ **Caption Updates** | 2-second intervals | Refresh frequency |

### ğŸš€ Optimization Features

<details>
<summary>Performance enhancements</summary>

- âš¡ **Adaptive Frame Skipping**: Dynamically adjusts based on processing speed
- ğŸ¯ **Scene Change Detection**: Reduces redundant processing  
- ğŸ’¾ **Smart Caching**: Reuses captions for similar scenes
- ğŸ“¦ **Batch Processing**: Efficient tensor operations
- ğŸ§¹ **Memory Management**: Automatic cleanup and garbage collection

</details>

---

## ğŸ›¡ï¸ API Reference

### ğŸŒ REST Endpoints

<details>
<summary>Available endpoints</summary>

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | ğŸ  Main application interface |
| `GET` | `/status` | ğŸ“Š Server status and configuration |
| `GET` | `/metrics` | ğŸ“ˆ Detailed performance metrics |
| `POST` | `/clear_cache` | ğŸ—‘ï¸ Reset caption cache |
| `GET\|POST` | `/config` | âš™ï¸ Dynamic configuration management |

</details>

### ğŸ”Œ WebSocket Events

<details>
<summary>Real-time communication</summary>

| Event | Direction | Description |
|-------|-----------|-------------|
| `connect` | ğŸ“¥ Server | Client connection established |
| `disconnect` | ğŸ“¥ Server | Client disconnection |
| `image` | ğŸ“¤ Client | Frame data transmission |
| `caption` | ğŸ“¥ Server | Caption response with metadata |

</details>

---

## ğŸ”§ Troubleshooting

### ğŸš¨ Common Issues

<details>
<summary>ğŸ“¹ Webcam Access Denied</summary>

**Solutions:**
- âœ… Ensure browser permissions are granted
- âš™ï¸ Check system privacy settings  
- ğŸ”„ Verify webcam is not in use by other applications
- ğŸŒ Try a different browser

</details>

<details>
<summary>âš¡ Poor Performance</summary>

**Solutions:**  
- ğŸ“Š Reduce frame rate in settings
- ğŸ¨ Lower image quality setting
- ğŸ® Check GPU availability and drivers
- ğŸ“Š Monitor system resource usage

</details>

<details>
<summary>ğŸ”Œ Connection Issues</summary>

**Solutions:**
- âœ… Verify server is running on correct port
- ğŸ›¡ï¸ Check firewall settings
- ğŸŒ Ensure WebSocket support in browser  
- ğŸ” Review browser console for errors

</details>

<details>
<summary>ğŸ“ Caption Quality Issues</summary>

**Solutions:**
- ğŸ’¡ Ensure adequate lighting conditions
- ğŸ¯ Adjust confidence threshold in settings
- ğŸ—‘ï¸ Clear cache to reset model state
- âœ… Verify BLIP model downloaded correctly

</details>

### ğŸ› Debug Mode
```python
# Enable debug logging
app.debug = True
socketio.run(app, debug=True)
```

---

## ğŸ¤ Contributing

### ğŸ› ï¸ Development Setup

<details>
<summary>Get started with development</summary>

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create feature branch  
3. ğŸ“¦ Install development dependencies
4. âœ… Make changes with tests
5. ğŸ“¤ Submit pull request

</details>

### ğŸ“ Code Style
- ğŸ Follow PEP 8 for Python code
- ğŸ“ Use meaningful variable names  
- ğŸ“š Add docstrings for functions
- ğŸ¨ Maintain consistent formatting

---

## ğŸ™ Acknowledgments

- ğŸ¤– **BLIP Model**: [Salesforce Research team](https://github.com/salesforce/BLIP)
- ğŸ”¥ **PyTorch**: [Facebook AI Research](https://pytorch.org)
- ğŸŒ **Flask-SocketIO**: [Miguel Grinberg](https://github.com/miguelgrinberg/Flask-SocketIO)
- ğŸ¤— **Transformers**: [Hugging Face team](https://huggingface.co/transformers)

---

## ğŸš€ Future Enhancements

<details>
<summary>Roadmap for upcoming features</summary>

- ğŸŒ **Multi-language caption support**
- ğŸ¯ **Object detection integration**  
- â˜ï¸ **Cloud deployment options**
- ğŸ“± **Mobile app development**
- ğŸ”„ **Real-time translation features**
- â™¿ **Enhanced accessibility options**

</details>

---

## ğŸ’¬ Support

<details>
<summary>Get help and support</summary>

- ğŸ› **Bug Reports**: [Open an issue](https://github.com/Varsha-1605/Real-Time-Video-Captioning-Project/issues)
- ğŸ’¡ **Feature Requests**: [Start a discussion](https://github.com/Varsha-1605/Real-Time-Video-Captioning-Project/discussions)
- ğŸ“§ **Contact**: [Email me](mailto:varshadewangan1605@gmail.com)
- ğŸ“š **Documentation**: [Visit the repository](https://github.com/Varsha-1605/Real-Time-Video-Captioning-Project)

</details>

---

<div align="center">

### ğŸŒŸ Star this repo if you found it helpful!

**Made with â¤ï¸ by Varsha**

[![GitHub stars](https://img.shields.io/github/stars/Varsha-1605/Real-Time-Video-Captioning-Project.svg?style=social&label=Star)](https://github.com/Varsha-1605/Real-Time-Video-Captioning-Project/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/Varsha-1605/Real-Time-Video-Captioning-Project.svg?style=social&label=Fork)](https://github.com/Varsha-1605/Real-Time-Video-Captioning-Project/network)

</div>
